{
  "cells": [
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T15:37:44.243259Z",
          "start_time": "2024-11-10T15:37:44.022047Z"
        },
        "id": "cFqgW0Ps7UVN"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec, Normalizer, BucketedRandomProjectionLSH\n",
        "from pyspark.ml.tuning import ParamGridBuilder"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OOhG_eZj7UVR"
      },
      "cell_type": "markdown",
      "source": [
        "### Load data\n",
        "___"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T15:48:58.449224Z",
          "start_time": "2024-11-10T15:48:57.449490Z"
        },
        "id": "PKPHieBc7UVS"
      },
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"HW1 MMDS\").getOrCreate()\n",
        "\n",
        "# Airbnb\n",
        "listings_df = spark.read.options(header=True, multiline=True, escape='\"').csv(\"listings.csv.gz\", inferSchema=True)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec = Window.partitionBy(\"name\").orderBy(\"name\")\n",
        "df_with_row_num = listings_df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
        "duplicates = df_with_row_num.filter(col(\"row_number\") > 1)\n",
        "duplicates.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ax3YLOxBM6B",
        "outputId": "c0bc9962-3bf0-4ac5-f8a3-b5a30cb912c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "788"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We assign a row number within each partition of identical names and filter out the first occurrence (row_number > 1) to isolate and count the duplicates."
      ],
      "metadata": {
        "id": "PC_nBbLMEJZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicates can hinder LSH by overloading hash buckets with identical or very similar items, reducing the algorithm's ability to distinguish between unique data points."
      ],
      "metadata": {
        "id": "gNHRpX34Cv2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df = listings_df.drop_duplicates([\"name\"]).select(\"name\")\n",
        "\n",
        "# Wiki\n",
        "url = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/uk.wikisource/all-access/2019/04/all-days'\n",
        "response = requests.get(url, headers={'User-Agent': 'MyPythonApp/1.0'})\n",
        "\n",
        "articles = [\n",
        "    article['article']\n",
        "    for item in response.json()['items']\n",
        "    for article in item['articles']\n",
        "]\n"
      ],
      "metadata": {
        "id": "JNLGn439BKW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILKRvpvnFNnR",
        "outputId": "ed87c1e1-288d-4fab-f006-9c8ca691161d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Головна_сторінка',\n",
              " 'Вірую',\n",
              " 'Мойсей_(Іван_Франко)/Пролог',\n",
              " 'Закон_України_«Про_авторське_право_і_суміжні_права»',\n",
              " 'Закон_України_«Про_Національну_поліцію»',\n",
              " 'Конституція_України',\n",
              " 'Архіви/ДАЖО/178/3',\n",
              " 'Конституція_Пилипа_Орлика',\n",
              " 'Конституція_США',\n",
              " 'Молитва_за_померлих']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles_df = spark.createDataFrame(\n",
        "    [(a.replace('_', ' ').lower(),) for a in articles],\n",
        "    [\"name\"]\n",
        ").filter(\"name != ''\").drop_duplicates([\"name\"])"
      ],
      "metadata": {
        "id": "rfwtqq4fEQ94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnrPmj1B7UVT"
      },
      "cell_type": "markdown",
      "source": [
        "### Define general pipeline\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have decided to use word2vec instead of tfidf, since it captures semantic meaning with vectors with lower amount of dimensions. Semantic meaning allows search based not only by the occurence of words, but also using the contextual info. Lower dimensionality in vectors reduces computational complexity. Normalizing Word2Vec vectors ensures that their magnitudes do not bias similarity measures"
      ],
      "metadata": {
        "id": "2yQ5RKecIQqf"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T15:49:02.957314Z",
          "start_time": "2024-11-10T15:49:02.930533Z"
        },
        "id": "UFFxNcko7UVT"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"words\")\n",
        "word2Vec = Word2Vec(inputCol=\"words\", outputCol=\"word2vec\", minCount=0)\n",
        "word2vec_normalizer = Normalizer(inputCol=\"word2vec\", outputCol=\"NormalizedWord2Vec\", p=2)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, word2Vec, word2vec_normalizer])\n",
        "\n",
        "brpLSH = BucketedRandomProjectionLSH(inputCol=\"NormalizedWord2Vec\", outputCol=\"hashes\")\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        "            .addGrid(brpLSH.numHashTables, [1, 3, 5])\n",
        "            .addGrid(brpLSH.bucketLength, [0.5, 1.0, 2.0])\n",
        "            .build())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "XLEM1VE87UVT"
      },
      "cell_type": "markdown",
      "source": [
        "### Define evaluation framework\n",
        "___"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T15:49:03.952413Z",
          "start_time": "2024-11-10T15:49:03.947856Z"
        },
        "id": "nkY05UbC7UVT"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def track_time(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        print(f\"Execution time of {func.__name__}: {execution_time:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T16:14:56.440026Z",
          "start_time": "2024-11-10T16:14:56.434763Z"
        },
        "id": "whQ69ZXV7UVT"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model_with_euclidean_distance(model, df, num_query_points=50, num_neighbors=250):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a LSH model by comparing its\n",
        "    approximate nearest neighbors to the exact nearest neighbors based on Euclidean distance.\n",
        "\n",
        "    Parameters:\n",
        "    - model (BucketedRandomProjectionLSH): pretrained model.\n",
        "    - df (Dataframe): contains at least the columns 'name' and 'NormalizedWord2Vec'.\n",
        "    - num_query_points (int, optional): Number of query points to sample for evaluation. Default is 50.\n",
        "    - num_neighbors (int, optional): Number of nearest neighbors to retrieve for each query point. Default is 250.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average similarity precision over all query points. The similarity score for each query point\n",
        "      is the fraction of common neighbors between the approximate nearest neighbors returned by the LSH model\n",
        "      and the exact nearest neighbors computed using Euclidean distance on word2vec embeddings.\n",
        "    \"\"\"\n",
        "    query_points = df.select(\"name\", \"NormalizedWord2Vec\").sample(False, 0.1).limit(num_query_points).collect()\n",
        "    similarity_results = []\n",
        "\n",
        "    for query_point in query_points:\n",
        "        key_vector = query_point[\"NormalizedWord2Vec\"]\n",
        "\n",
        "        compute_l2_distance_udf = udf(lambda vec: float(np.linalg.norm(vec - key_vector)), FloatType())\n",
        "\n",
        "        # Get top real nearest neighbors based on Euclidean distance (excluding the query point)\n",
        "        real_neighbors = (df.withColumn(\"distCol\", compute_l2_distance_udf(col(\"NormalizedWord2Vec\")))\n",
        "                          .filter(f\"distCol > 0\")\n",
        "                          .orderBy(\"distCol\")\n",
        "                          .limit(num_neighbors)\n",
        "                          .select(\"name\")\n",
        "                          .collect())\n",
        "\n",
        "        # Get top approximate nearest neighbors from the LSH model\n",
        "        ann_neighbors = (model.approxNearestNeighbors(df, key_vector, num_neighbors + 1)\n",
        "                         .filter(\"distCol > 0\")\n",
        "                         .select(\"name\")\n",
        "                         .collect())\n",
        "\n",
        "        real_neighbor_names = {neighbor[\"name\"] for neighbor in real_neighbors}\n",
        "        ann_neighbor_names = {neighbor[\"name\"] for neighbor in ann_neighbors}\n",
        "\n",
        "        similarity = len(real_neighbor_names & ann_neighbor_names) / num_neighbors\n",
        "        similarity_results.append(similarity)\n",
        "\n",
        "    return np.mean(similarity_results)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T16:14:57.547135Z",
          "start_time": "2024-11-10T16:14:57.542311Z"
        },
        "id": "QjZ2daWp7UVT"
      },
      "cell_type": "code",
      "source": [
        "@track_time\n",
        "def tune_params(df):\n",
        "    \"\"\"\n",
        "    Performs hyperparameter tuning for the BucketedRandomProjectionLSH (LSH) model using a predefined parameter grid,\n",
        "    and selects the best model based on the highest average similarity score.\n",
        "\n",
        "    The function:\n",
        "    - Transforms the input DataFrame using a predefined pipeline.\n",
        "    - Iterates over a grid of LSH parameters (`numHashTables` and `bucketLength`).\n",
        "    - Fits the LSH model with each parameter combination on the transformed data.\n",
        "    - Evaluates each model using the `evaluate_model_with_euclidean_distance` function, which computes the average recall at k.\n",
        "    - Identifies and retains the model with the highest average similarity score.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pyspark.sql.DataFrame): The input DataFrame containing the data to be used for tuning.\n",
        "      It should include at least the columns required by the pipeline (e.g., 'name').\n",
        "\n",
        "    Side Effects:\n",
        "    - Prints evaluation results for each parameter combination, including the parameters used and the average similarity score.\n",
        "    - Prints the best parameters found and the highest average similarity score.\n",
        "\n",
        "    \"\"\"\n",
        "    transformed_data = pipeline.fit(df).transform(df)\n",
        "\n",
        "    best_model, best_params = None, None\n",
        "    highest_avg_similarity = 0.0\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    for params in paramGrid:\n",
        "        brpLSH.setParams(numHashTables=params[brpLSH.numHashTables], bucketLength=params[brpLSH.bucketLength])\n",
        "        model = brpLSH.fit(transformed_data)\n",
        "\n",
        "        avg_similarity = evaluate_model_with_euclidean_distance(model, transformed_data)\n",
        "\n",
        "        print(f\"numHashTables={params[brpLSH.numHashTables]}, bucketLength={params[brpLSH.bucketLength]}\")\n",
        "        print(f\"Average Similarity: {avg_similarity}\")\n",
        "\n",
        "        if avg_similarity > highest_avg_similarity:\n",
        "            highest_avg_similarity = avg_similarity\n",
        "            best_model, best_params = model, params\n",
        "\n",
        "    print(f\"\\nBest Parameters:\")\n",
        "    print(f\"numHashTables={best_params[brpLSH.numHashTables]}, bucketLength={best_params[brpLSH.bucketLength]}\")\n",
        "    print(f\"Highest Average Similarity: {highest_avg_similarity}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "TWMHA7jW7UVU"
      },
      "cell_type": "markdown",
      "source": [
        "### AirBnb param tuning\n",
        "___"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T16:31:32.762261Z",
          "start_time": "2024-11-10T16:14:58.516445Z"
        },
        "id": "iPDwPh4l7UVU",
        "outputId": "a59366c2-7076-4b55-8f91-796dae5db219"
      },
      "cell_type": "code",
      "source": [
        "tune_params(names_df)"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=1, bucketLength=0.5\n",
            "Average Similarity: 0.84112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=1, bucketLength=1.0\n",
            "Average Similarity: 0.8613599999999999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=1, bucketLength=2.0\n",
            "Average Similarity: 0.80688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=3, bucketLength=0.5\n",
            "Average Similarity: 0.9878399999999999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=3, bucketLength=1.0\n",
            "Average Similarity: 0.99424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=3, bucketLength=2.0\n",
            "Average Similarity: 0.9847999999999999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=5, bucketLength=0.5\n",
            "Average Similarity: 0.9995200000000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=5, bucketLength=1.0\n",
            "Average Similarity: 0.99976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=5, bucketLength=2.0\n",
            "Average Similarity: 0.99936\n",
            "\n",
            "Best Parameters:\n",
            "numHashTables=5, bucketLength=1.0\n",
            "Highest Average Similarity: 0.99976\n",
            "Execution time of tune_params: 994.2438 seconds\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qVCeewiZ7UVU"
      },
      "cell_type": "markdown",
      "source": [
        "### Wiki param tuning\n",
        "___\n"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T14:31:04.784525Z",
          "start_time": "2024-11-10T14:30:36.432390Z"
        },
        "id": "Y4R03I6b7UVV",
        "outputId": "89b403d7-ddab-4f56-e8e0-0cfc51bf08a3"
      },
      "cell_type": "code",
      "source": [
        "tune_params(articles_df)"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numHashTables=1, bucketLength=0.5\n",
            "Average Similarity: 0.68\n",
            "numHashTables=1, bucketLength=1.0\n",
            "Average Similarity: 0.68\n",
            "numHashTables=1, bucketLength=2.0\n",
            "Average Similarity: 0.72\n",
            "numHashTables=3, bucketLength=0.5\n",
            "Average Similarity: 0.96\n",
            "numHashTables=3, bucketLength=1.0\n",
            "Average Similarity: 0.9199999999999999\n",
            "numHashTables=3, bucketLength=2.0\n",
            "Average Similarity: 0.9199999999999999\n",
            "numHashTables=5, bucketLength=0.5\n",
            "Average Similarity: 1.0\n",
            "numHashTables=5, bucketLength=1.0\n",
            "Average Similarity: 1.0\n",
            "numHashTables=5, bucketLength=2.0\n",
            "Average Similarity: 1.0\n",
            "\n",
            "Best Parameters:\n",
            "numHashTables=5, bucketLength=0.5\n",
            "Highest Average Similarity: 1.0\n",
            "Execution time of tune_params: 28.3503 seconds\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c5eqHghd7UVV"
      },
      "cell_type": "markdown",
      "source": [
        "### Wiki with Airbnb params\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  We can notice that although the parameters were tuned on the airbnb dataset, we were still able to achieve considerable performance. This could be due to the fact that we try a big amount of hashtables, that improves recall and would perform better for any dataset, although we some other parameter set still performs better in this particular case.\n",
        "\n"
      ],
      "metadata": {
        "id": "42rI_EopOd_a"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-10T14:33:13.757290Z",
          "start_time": "2024-11-10T14:33:10.124139Z"
        },
        "id": "GawJKcn97UVV",
        "outputId": "4cf95ff4-0aa6-4fcb-b79f-96516a4c3d23"
      },
      "cell_type": "code",
      "source": [
        "transformed_data = pipeline.fit(articles_df).transform(articles_df)\n",
        "\n",
        "brpLSH = BucketedRandomProjectionLSH(inputCol=\"NormalizedWord2Vec\", outputCol=\"hashes\", numHashTables=3, bucketLength=0.5)\n",
        "model = brpLSH.fit(transformed_data)\n",
        "\n",
        "average_accuracy = evaluate_model_with_euclidean_distance(model, transformed_data)\n",
        "print(f\"\\nAverage KNN-ANN Accuracy with Airbnb Parameters: {average_accuracy}\")\n",
        "\n",
        "sample_vector = transformed_data.select(\"NormalizedWord2Vec\").limit(1).collect()[0][0]\n",
        "similar_articles = model.approxNearestNeighbors(transformed_data, sample_vector, 6)\n",
        "similar_articles.select(\"name\", \"distCol\").filter(\"distCol > 0\").show()"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average KNN-ANN Accuracy with Airbnb Parameters: 0.9199999999999999\n",
            "+--------------------+------------------+\n",
            "|                name|           distCol|\n",
            "+--------------------+------------------+\n",
            "|Сторінка:Твори_Ко...|1.1629896902350065|\n",
            "|Будапештський_мем...|1.2045824380578187|\n",
            "|Зернятка_(Борис_Г...|1.2058485606076885|\n",
            "|Вікіджерела:Прави...|1.2114045430846063|\n",
            "|Перша_Французька_...|1.2270553489447011|\n",
            "+--------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System characteristics\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ujx0Rb5BP9bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have use Google Colab for running this experiments. We've used CPU (Intel(R) Xeon(R) with 2 cores) with 12 GB's of RAM. You can observe the detailed description of the hardware below"
      ],
      "metadata": {
        "id": "I3nNxmPCEahP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CPU Information:\")\n",
        "!lscpu\n",
        "\n",
        "print(\"\\nMemory Information:\")\n",
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjGmHyEQGMH",
        "outputId": "eadab2df-40e6-4481-acbe-68d3f4e9ad84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Information:\n",
            "Architecture:             x86_64\n",
            "  CPU op-mode(s):         32-bit, 64-bit\n",
            "  Address sizes:          46 bits physical, 48 bits virtual\n",
            "  Byte Order:             Little Endian\n",
            "CPU(s):                   2\n",
            "  On-line CPU(s) list:    0,1\n",
            "Vendor ID:                GenuineIntel\n",
            "  Model name:             Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:           6\n",
            "    Model:                79\n",
            "    Thread(s) per core:   2\n",
            "    Core(s) per socket:   1\n",
            "    Socket(s):            1\n",
            "    Stepping:             0\n",
            "    BogoMIPS:             4399.99\n",
            "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 cl\n",
            "                          flush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc re\n",
            "                          p_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3\n",
            "                           fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand\n",
            "                           hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp \n",
            "                          fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx sm\n",
            "                          ap xsaveopt arat md_clear arch_capabilities\n",
            "Virtualization features:  \n",
            "  Hypervisor vendor:      KVM\n",
            "  Virtualization type:    full\n",
            "Caches (sum of all):      \n",
            "  L1d:                    32 KiB (1 instance)\n",
            "  L1i:                    32 KiB (1 instance)\n",
            "  L2:                     256 KiB (1 instance)\n",
            "  L3:                     55 MiB (1 instance)\n",
            "NUMA:                     \n",
            "  NUMA node(s):           1\n",
            "  NUMA node0 CPU(s):      0,1\n",
            "Vulnerabilities:          \n",
            "  Gather data sampling:   Not affected\n",
            "  Itlb multihit:          Not affected\n",
            "  L1tf:                   Mitigation; PTE Inversion\n",
            "  Mds:                    Vulnerable; SMT Host state unknown\n",
            "  Meltdown:               Vulnerable\n",
            "  Mmio stale data:        Vulnerable\n",
            "  Reg file data sampling: Not affected\n",
            "  Retbleed:               Vulnerable\n",
            "  Spec rstack overflow:   Not affected\n",
            "  Spec store bypass:      Vulnerable\n",
            "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swa\n",
            "                          pgs barriers\n",
            "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BH\n",
            "                          I: Vulnerable (Syscall hardening enabled)\n",
            "  Srbds:                  Not affected\n",
            "  Tsx async abort:        Vulnerable\n",
            "\n",
            "Memory Information:\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       1.5Gi       7.3Gi       2.0Mi       3.9Gi        10Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}